# Web Crawler

### Usage
* Local:
	* Install external dependencies: `pip install -r requirements.txt`
	* Run the script: `<python_3.10+_interpreter> main.py`
* Docker:
	* Build image: `docker build -t crawler .`
	* Run it: `docker run -it crawler`

## Project Design
![](https://i.ibb.co/wSSKZMc/web-crawler-project-diagram.png)

## File Structure
*  **configs** -> Config files. In a real-life project, the config files shouldn't be in the project directory, but instead in some sort of external storage, like S3.
*  **crawlers** -> Web crawlers.
*  **db** -> DBs.
*  **handlers** -> Provide generic functionality without dependency on specific technologies. For example - parsing html elements.
*  **store** -> Store the local files (generated by the local storage database).
*  **exceptions.py** -> Custom exceptions.
*  **main.py** -> The project's entry point.
## How I Managed The Assignment

### The Basic Requirements
* learned the basics of python.
* researched web crawlers - how does it work? what are the ways to implement it? (scrapy, beautifulSoup, etc).
* researched the other libraries that were recommended in the home assignment description.
* created a basic design for this assignment - I wanted to create something that is reusable, clean, easy to modify or extend, and that is not bound to the specific libraries I chose to use. You can see the design diagram [here](#project-design).
#### Crawlers
I created an abstract class `Crawler` that contains general functionality that needs to be implemented.
`PastebinCrawler` class inherits from it.
The process begins when the program calls the `process` method.
##### Notes
* I chose to use sets for creating `to_be_visited` and `visited` because of its O(1) time complexity searching (which is mostly needed for `visited`), and if the same link is mistakenly added twice, it won't mess things up.
* The database instance is passed to the constructor, so it gives us the convenience of using any database we want.
#### Databases
I created an abstract class `DB`.
All database implementations should implement it.
#### Handlers
Each handler is a "wrapper" for specific functionality that is needed in the project.
Things like parsing HTML, requesting data from the web, creating jobs -> there are multiple ways to implement each one, and it can be changed in the future.
#### Threading
Each time the web crawler asks for new data, it needs to handle up to 50 records (mostly on the first fetch). To handle this amount of data, it's fetching, processing, and writing is done using multithreading. The number of workers is limited in order to not overload the endpoint with requests and end up getting rejected.

#### Configs
I created a config with all the constants that were required in the assignment + general data about creating the data model. It was created so it will be simple to change the values, organize the access to the data, and create uniformity in the code.

#### Data Model
I added a fifth property - `id` for making my life easier while creating local files.
The id is the file name and the record's unique URL route on the website.

#### Notes
The best way I found to get the Pastebin pastes, was through this URL - [https://pastebin.com/archive](https://pastebin.com/archive) -> it contains the last 50 public paste posts. So I used this URL for getting all the Pastebin posts' links.

## Bonuses
I was focused on the basic requirements of the assignments, so I managed to complete only the #3 bonus.
However, I want to point out how I would implement the other bonuses.
#### Bonus #1
* In `PastebinCrawler` Class -> Create `normalize_data_model` function, and call it in the `get_link_data` function
* How I would handle each property normalization?
	* Author & Title
		* Search if there is a summary somewhere online of the generic naming for each property.
		* If not, I would look at different kinds of paste posts and try to locate them manually.
		* I would probably store those generic values in `pastebin_config.data_model_to_extract[property]`, with the normalized value that I would choose.
		* In the function, I would get the data from the config and change the values accordingly.
	* Date -> use arrow library to convert the date to UTC Date.
	* Content -> use the `Str.strip()` function.

#### Bonus #2
* Create a class for the external database, that inherits from `DB` class and implements the correct methods.
* Create an instance of the class in `main.py` and use it.
## Notes
* Just before sending the assignment, I thought it might be a good idea to create a job that deletes the `PatebinCrawler.visited` data every 24 hours or so. Because otherwise, the set might accumulate large amounts of "expired" data. However, I realized it would require me to change the logic in `job_handler`, so I didn't implement it.
* I enjoyed creating this project. It was fun to create something new that is so different from what I have done so far. In a new programming language, I never experienced before. Also, the web crawlers concept is so cool.
* I will appreciate it if you could give me feedback. As I mentioned, I have never done something like that before, and I want to know if I created it correctly.